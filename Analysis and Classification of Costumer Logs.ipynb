{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Networks Final Project\n",
    "\n",
    "Maxim Torubarov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I analyze customer complaints, requests, and queries that are send to a company. The company keeps track of these messages as tickets. All of the tickets (64016 of them) are kept in tickets_details.csv which consists of rows of tickets each with 7 different columns: Ticket ID, Subject, Description, Status, Type, Created time, and Contact_id. \n",
    "-\tTicket ID is a unique identifier for each ticket. \n",
    "-\tSubject is the subject of the message. \n",
    "-\tDescription is the main message that the customers write. \n",
    "-\tStatus is either Closed, Resolved, or Open and indicates whether or not the company has dealt with the ticket\n",
    "-\tType is the type of ticket\n",
    "-\tCreated time is the time when the ticket was created\n",
    "-\tContact_id is the customer’s id. There may be more than 1 ticket per Contact_id\n",
    "\n",
    "The tickets span from 5/8/17 through 2/26/18.\n",
    "My project is split into 3 parts: classification, sentiment analysis, and analytics. We proceed by describing the first part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part was to classify the descriptions into types. So, for example, a description talking about cancelling an order should be classified as a “Classification” type. There are a total of 38 classes (including a few I get rid off later) which is a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import operator\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classifying anything, I had to get the data from the file and preprocess it. I went row by row in the data and for each element in the row (corresponding to a column), I put the element in one of the 7 arrays (one for each column). In this step, I also took care of several bad rows. Namely, I ignored any row with an empty string in the type column or the description column since these rows don’t really hold any useful information. I also found another row with “binalg@yahoo.com” in its type column. This was the only row with this type and the string doesn’t mean anything, so I took out the row as well. Next, I looked at the types and noticed that some of them are very similar to one another at least by name. These similar types and how I dealt with them are as follows:\n",
    "-\t“Cancellation” and “Cancellation// RTO”: I noticed that descriptions having this type were generally very similar in what they were saying: for both, the descriptions are requests to cancel some order. Furthermore, there were only 49 cases of “Cancellation” type whereas there were 1660 cases of the other type. I figured that both these types were categorizing very similar if not the same things so for every row with “Cancellation// RTO” type, I changed it to “Cancellation type”. Perhaps there are two classifications here because the company has some other knowledge on the ticket that this excel document doesn’t provide us, like what state the order is currently in.\n",
    "-\t“Offer Query” and “Offer Related”: “Offer Query” appears only until 11/5/2017 after which it doesn’t appear again. After this, only “Offer Related” is used. Once again, descriptions in both types appeared to be very similar. Furthermore, “Offer Query” appeared only 7 times as opposed to 70 so it’s not like they were trying to describe two different common types of descriptions. It appears as if the human annotators used both at first, and then realized they were the same thing and only used “Offer Related” from then on. So like before, I changed each “Offer Query” entry to “Offer Related”\n",
    "-\t“Order Related” and “Order Related Request”: Analyzing descriptions of both types, I noticed that they were very similar if not the same. Because, the “Order Related” type was used significantly more than the other tag(26248 vs 414), I figured that “Order Related Request” was just a subset of “Order Related”. I joined both of these tags by changing each “Order Related Request” tag to “Order Related”.\n",
    "\n",
    "I found that these small changes in types produced good improvements in the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arrays that store the data from each of the columns\n",
    "ticketIDs = []\n",
    "subjects = []\n",
    "descriptions = []\n",
    "statuses = []\n",
    "types = []\n",
    "createdTimes = []\n",
    "contactIds = []\n",
    "\n",
    "#open the file and preprocess the data\n",
    "with open ('tickets_details.csv',newline = '') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        #ignore rows with \"\" as types or descriptions\n",
    "        if(row[2]==\"\" or row[4]==\"\"):\n",
    "            continue\n",
    "            \n",
    "        #ignore the 1 row with an email address for type\n",
    "        if(\"binalg\" in row[4]):\n",
    "            continue\n",
    "        \n",
    "        #join \"Cancellation\" and \"Cancellation //RTO\" types\n",
    "        if(\"Cancellation\" in row[4][:13]):\n",
    "            row[4] = 'Cancellation'\n",
    "            \n",
    "        #join \"Offer Query\" and \"Offer Related\" types\n",
    "        if(\"Offer\" in row[4][:5]):\n",
    "            row[4]=\"Offer Related\"\n",
    "            \n",
    "        #join \"Order Related\" and \"Order Related Request\"\n",
    "        if(\"Order Related\" in row[4]):\n",
    "            row[4]=\"Order Related\" \n",
    "        \n",
    "        #add the elements to the corresponding arrays\n",
    "        ticketIDs.append(row[0])\n",
    "        subjects.append(row[1])\n",
    "        descriptions.append(row[2])\n",
    "        statuses.append(row[3])\n",
    "        types.append(row[4])\n",
    "        createdTimes.append(row[5])\n",
    "        contactIds.append(row[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is the classes of things that the descriptions can be\n",
    "classes = list(set(types))\n",
    "classes = sorted(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle all the data around\n",
    "indeces = [i for i in range(len(descriptions))]\n",
    "\n",
    "random.shuffle(indeces)\n",
    "ticketIDs = [ticketIDs[i] for i in indeces]\n",
    "subjects= [subjects[i] for i in indeces]\n",
    "descriptions= [descriptions[i] for i in indeces]\n",
    "statuses= [statuses[i] for i in indeces]\n",
    "types= [types[i] for i in indeces]\n",
    "createdTimes= [createdTimes[i] for i in indeces]\n",
    "contactIds= [contactIds[i] for i in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Preprocessing: lowercasing, getting rid of punctuations, spell checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge difficulty with this data is the amount of spelling errors in the descriptions. Many common English words were misspelled which is problematic for a classifier which sees a misspelled word and its correct spelling as two different words. To deal with this, I first create a vocabulary of all the different words in the description. Then I correct some of the mispelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method that given some text, splits it up into word tokens\n",
    "def getWordsList(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "#stores each element of descriptions list as a list of words instead of a full sentence\n",
    "descriptionsWords = []\n",
    "\n",
    "#a Counter of all the words in the descriptions and the number of times they occur\n",
    "vocabulary=Counter()\n",
    "\n",
    "#get descriptionWords and vocabulary\n",
    "for entry in descriptions:\n",
    "    allWords = getWordsList(entry)\n",
    "    descriptionsWords.append(allWords)\n",
    "    for word in allWords:\n",
    "        if(word in vocabulary):\n",
    "            vocabulary[word] = vocabulary[word] + 1\n",
    "        else:\n",
    "            vocabulary[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if a word appears less than this threshold, that means that it is very unlikely to be a correct spelling\n",
    "threshold = 2\n",
    "\n",
    "#the total number of words in all the descriptions\n",
    "totalNumberWords= sum(vocabulary.values())\n",
    "\n",
    "#The following methods are inspired by http://norvig.com/spell-correct.html\n",
    "\n",
    "#the number of times the word appears in the descriptions / total number of words in the descriptions\n",
    "def probability(word):\n",
    "    return vocabulary[word]/totalNumberWords\n",
    "\n",
    "#the most likely spelling correction\n",
    "#returns dictionary of possible words that is sorted\n",
    "def correction(word): \n",
    "    #weights depending on type of word\n",
    "    probItself=50\n",
    "    probEdits1 = 4\n",
    "    probEdits2 = 1\n",
    "    \n",
    "    possibleCandidates = {}\n",
    "    \n",
    "    probOfThisWord = probability(word)\n",
    "    if(probOfThisWord == 0 ):\n",
    "        probOfThisWord = 1/totalNumberWords\n",
    "    possibleCandidates[word]=probOfThisWord*probItself\n",
    "    \n",
    "    for w in edits1(word):\n",
    "        if w not in possibleCandidates:\n",
    "            possibleCandidates[w] = probability(w)*probEdits1\n",
    "    for w in edits2(word):\n",
    "        if w not in possibleCandidates:\n",
    "            possibleCandidates[w] = probability(w)*probEdits2\n",
    "\n",
    "    sortedDict = collections.OrderedDict(sorted(possibleCandidates.items(),key=operator.itemgetter(1),reverse=True))#sorted(possibleCandidates.items(), key = lambda(k,v): (v,k))\n",
    "    \n",
    "    return dict(sortedDict)\n",
    "\n",
    "#returns candidates for the word\n",
    "def candidates(word): \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "    \n",
    "#Returns the words that appear in the vocabulary\n",
    "def known(words): \n",
    "    return set(w for w in words if w in vocabulary and vocabulary[w]>=threshold)\n",
    "\n",
    "#Returns all words that are 1 edit away from the input word\n",
    "def edits1(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "#Returns all words that are 2 edits away from the input word\n",
    "def edits2(word): \n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following takes a while so if you don't want to do the spellchecking feel free to make \"doSpellChecking\" = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doSpellChecking = False\n",
    "if(doSpellChecking):\n",
    "\n",
    "    #we go through every entry: make it lowercase, and correct spelling\n",
    "    progress = 0\n",
    "    newDescriptions = []\n",
    "    for index,entry in enumerate(descriptions):\n",
    "        words = descriptionsWords[index]\n",
    "        updatedWords = \"\"\n",
    "        for word in words:\n",
    "            updatedWords = updatedWords+ \" \" + list(correction(word).keys())[0]  #list(my_dict.keys())[0]\n",
    "        newDescriptions.append(updatedWords)\n",
    "        progress+=1\n",
    "        if(progress%10 == 0):\n",
    "            print(\"p:\",progress)\n",
    "    descriptions = newDescriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXPERIMENT:\n",
    "#using just descriptions to classify into types\n",
    "\n",
    "\n",
    "#first get the test, dev, train split: 70% is training, 30% is split into mix(30% dev, 70% test)\n",
    "#62000 rows in total\n",
    "trainingPercentage = .8#.7\n",
    "devPercentage = .8+.2*.2#.79\n",
    "\n",
    "\n",
    "size=len(descriptions)\n",
    "ttrain_x=descriptions[:int(trainingPercentage*size)]\n",
    "ttrain_y=types[:int(trainingPercentage*size)]\n",
    "tdev_x=descriptions[int(trainingPercentage*size):int(devPercentage*size)]\n",
    "tdev_y=types[int(trainingPercentage*size):int(devPercentage*size)]\n",
    "ttest_x=descriptions[int(devPercentage*size):]\n",
    "ttest_y=types[int(devPercentage*size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Around with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example method I used to look at the data based on type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for data with type: Address Mismatch\n",
      "this type appears:  2  times\n",
      "------------------\n",
      " Please write above this line. Help us serve you better by describing your issue in detail. How can i change the address for the order placed by me which is confirmed \n",
      "------------------\n",
      "Order Id: 5a91553c1f2173243ed22d92 Please write above this line.272001 is wrong pin code 272173 is the right pin code plea sipd this address \n"
     ]
    }
   ],
   "source": [
    "#look at descriptions of specific type\n",
    "typeNum=1\n",
    "\n",
    "\n",
    "def getSpecificX(type, allX , allY):\n",
    "    print(\"looking for data with type: \" + type)\n",
    "    x = []\n",
    "    for i in range(len(allY)):\n",
    "        if(type == allY[i]):\n",
    "            x.append(allX[i])\n",
    "    return x\n",
    "\n",
    "#x = getSpecificX(classes[typeNum], ttrain_x, ttrain_y)\n",
    "x = getSpecificX(classes[typeNum], ttrain_x+tdev_x, ttrain_y+tdev_y)\n",
    "\n",
    "print(\"this type appears: \",len(x),\" times\")\n",
    "for e in x:\n",
    "    print(\"------------------\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data into a count vector, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect= CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(ttrain_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50177, 96549)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST size of training set\n",
    "X_train_counts.shape\n",
    "#first number is the amount of rows in the train\n",
    "#second number is the size of the vocabulary for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62722\n",
      "how many contact id's not unique: 45611\n"
     ]
    }
   ],
   "source": [
    "#TEST size of contact ids (not very interesting)\n",
    "print(len(contactIds))\n",
    "setContactIds = set(contactIds)\n",
    "print(\"how many contact id's not unique:\",len(contactIds)-len(setContactIds))    #45000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Order Related': 21398, 'Pickup Related': 6160, 'Delivery Related': 3640, 'Return/SBS': 3495, 'Payment Related': 2932, 'General Query': 2798, 'Other': 2089, 'SBS Related': 1689, 'Cancellation': 1371, 'Cash-Out Related': 1198, 'Feedback/ Feature Request': 928, 'Tech Related': 505, 'Ops Query': 355, 'Shipping Label': 221, 'Pre-sales queries': 208, 'Listing/Post Related': 181, 'Ops Complaint': 149, 'Account Related': 143, 'Refund Issue': 140, 'Damage': 85, 'Serviceability Query': 80, 'Referral': 79, 'App Related': 71, 'Offer Related': 65, 'Marketing Emails': 42, 'Post Sales queries': 41, 'Shipping Charges Queries': 36, 'Packaging Issues': 33, 'Career Related': 18, 'Privacy Violation': 17, 'Amount Debited order confirmed': 4, 'Bank Statement': 2, 'Address Mismatch': 2, 'SBS Request': 1, 'Test': 1})\n",
      "30\n",
      "['Return/SBS', 'Order Related', 'Ops Query', 'Delivery Related', 'General Query', 'Pickup Related', 'Payment Related', 'Cash-Out Related', 'Other', 'Cancellation', 'SBS Related', 'Refund Issue', 'Tech Related', 'Pre-sales queries', 'Listing/Post Related', 'Marketing Emails', 'Feedback/ Feature Request', 'Account Related', 'Referral', 'Post Sales queries', 'Offer Related', 'Serviceability Query', 'Ops Complaint', 'Shipping Label', 'Damage', 'Shipping Charges Queries', 'App Related', 'Career Related', 'Packaging Issues', 'Privacy Violation']\n"
     ]
    }
   ],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self, ntrain_x, ntrain_y, ndev_x, ndev_y, ntest_x, ntest_y):\n",
    "        self.train_x=list(ntrain_x)\n",
    "        self.train_y=list(ntrain_y)\n",
    "        self.classes = sorted(set(self.train_y))\n",
    "        self.dev_x=list(ndev_x)\n",
    "        self.dev_y=list(ndev_y)\n",
    "        self.test_x=list(ntest_x)\n",
    "        self.test_y=list(ntest_y)\n",
    "    \n",
    "    \n",
    "    def transformSoThatClassAppears(self, times):\n",
    "        \n",
    "        counter = Counter(self.train_y)\n",
    "        print(counter)\n",
    "        toKeep = []\n",
    "        for c in counter:\n",
    "            if(counter[c]<times):\n",
    "                pass\n",
    "            else:\n",
    "                toKeep.append(c)\n",
    "        print(len(toKeep))\n",
    "        print(toKeep)\n",
    "        index = 0\n",
    "        while index < len(self.train_y):\n",
    "            y = self.train_y[index]\n",
    "            if(y not in toKeep):\n",
    "                del self.train_x[index]\n",
    "                del self.train_y[index]\n",
    "            else:\n",
    "                index+=1\n",
    "        index=0\n",
    "        while index < len(self.dev_y):\n",
    "            y = self.dev_y[index]\n",
    "            if(y not in toKeep):\n",
    "                del self.dev_x[index]\n",
    "                del self.dev_y[index]\n",
    "            else:\n",
    "                index+=1\n",
    "        index=0\n",
    "        while index<len(self.test_y):\n",
    "            y = self.test_y[index]\n",
    "            if(y not in toKeep):\n",
    "                del self.test_x[index]\n",
    "                del self.test_y[index]\n",
    "            else:\n",
    "                index+=1\n",
    "        self.classes = sorted(set(self.train_y))\n",
    "    \n",
    "    #for following: test is if true, then we use test_x instead\n",
    "    #TODO: add this functionality\n",
    "    def predict(self,pipeline, test=False):\n",
    "        \n",
    "        pipeline.fit(self.train_x, self.train_y) \n",
    "        if(test==False):\n",
    "            dev_predicted = pipeline.predict(self.dev_x)\n",
    "            return dev_predicted\n",
    "        else:\n",
    "            test_predicted = pipeline.predict(self.test_x)\n",
    "            return test_predicted\n",
    "    \n",
    "    \n",
    "    \n",
    "    def printAccuracy(self,prediction, test=False):\n",
    "        if(test==False):\n",
    "            return np.mean(prediction==self.dev_y)\n",
    "        else:\n",
    "            return np.mean(prediction ==self.test_y)\n",
    "            \n",
    "    def printConfusionMatrix(self, prediction, test = False):\n",
    "        if(test==False):\n",
    "            print(metrics.classification_report(self.dev_y, prediction))\n",
    "        else:\n",
    "            print(metrics.classification_report(self.test_y, prediction))\n",
    "    \n",
    "e = Experiment(ttrain_x,ttrain_y,tdev_x,tdev_y,ttest_x,ttest_y)\n",
    "e.transformSoThatClassAppears(5) \n",
    "    #makes it so that we don't look at rows whose label is one that appears less than specified number of times in the testing data\n",
    "    #this method gets rid of types that don't appear often. Improves accuracy a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43946188340807174"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXPERIMENT 1: vect, tfidf, clf=MultinomialNB\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "])\n",
    "prediction = e.predict(pipeline, True)\n",
    "e.printAccuracy(prediction, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46447433981066266"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXPERIMENT 2: vect, clf=MultinomialNB\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "])\n",
    "prediction = e.predict(pipeline, True)\n",
    "e.printAccuracy(prediction, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49377179870453414"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXPERIMENT 3: vect, tfidf, clf=SGD\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=5, tol=None)),\n",
    "])\n",
    "prediction = e.predict(pipeline, True)\n",
    "e.printAccuracy(prediction, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50981564524165424"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXPERIMENT 4: vect, clf=SGD\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=5, tol=None)),\n",
    "])\n",
    "prediction = e.predict(pipeline, True)\n",
    "e.printAccuracy(prediction, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that count vectorizer does better without tfidf than with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "          Account Related       0.56      0.16      0.25        31\n",
      "              App Related       0.00      0.00      0.00        14\n",
      "             Cancellation       0.00      0.00      0.00       258\n",
      "           Career Related       0.00      0.00      0.00         7\n",
      "         Cash-Out Related       0.67      0.76      0.71       259\n",
      "                   Damage       0.33      0.05      0.09        19\n",
      "         Delivery Related       0.31      0.02      0.03       706\n",
      "Feedback/ Feature Request       0.34      0.14      0.19       176\n",
      "            General Query       0.41      0.13      0.20       551\n",
      "     Listing/Post Related       0.14      0.03      0.06        29\n",
      "         Marketing Emails       0.00      0.00      0.00         6\n",
      "            Offer Related       0.00      0.00      0.00         8\n",
      "            Ops Complaint       0.15      0.09      0.11        34\n",
      "                Ops Query       0.00      0.00      0.00        51\n",
      "            Order Related       0.52      0.85      0.65      4210\n",
      "                    Other       0.53      0.40      0.46       394\n",
      "         Packaging Issues       0.00      0.00      0.00         4\n",
      "          Payment Related       0.54      0.29      0.37       614\n",
      "           Pickup Related       0.47      0.52      0.49      1268\n",
      "       Post Sales queries       0.00      0.00      0.00        12\n",
      "        Pre-sales queries       0.12      0.02      0.04        41\n",
      "        Privacy Violation       0.00      0.00      0.00         3\n",
      "                 Referral       0.57      0.29      0.38        14\n",
      "             Refund Issue       0.00      0.00      0.00        26\n",
      "               Return/SBS       0.42      0.16      0.24       766\n",
      "              SBS Related       0.50      0.28      0.36       368\n",
      "     Serviceability Query       0.40      0.14      0.21        14\n",
      " Shipping Charges Queries       0.00      0.00      0.00        14\n",
      "           Shipping Label       0.20      0.02      0.04        51\n",
      "             Tech Related       0.53      0.18      0.27        87\n",
      "\n",
      "              avg / total       0.46      0.51      0.45     10035\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtoru\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "e.printConfusionMatrix(prediction, True)\n",
    "#support is the number of times that class appears in the y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test a sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_predicted = pipeline.predict(['I saw that my order was cancelled. Can you reorder it please'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Order Related'], \n",
       "      dtype='<U25')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We then do grid search to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try to see from experiment 4 all the posibilities\n",
    "parameters = {\n",
    "    'clf__alpha': (1e-1,1e-5),\n",
    "    'clf__max_iter': (5, 100),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__loss' : ('log','hinge', 'squared_hinge')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(e.train_x, e.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 1e-05\n",
      "clf__loss: 'log'\n",
      "clf__max_iter: 100\n",
      "clf__penalty: 'elasticnet'\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54170403587443949"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXPERIMENT 4: updated with grid search parameters\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('clf', SGDClassifier(loss='log', penalty='elasticnet',\n",
    "                                            alpha=1e-5, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "])\n",
    "prediction = e.predict(pipeline, True)\n",
    "e.printAccuracy(prediction, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "          Account Related       0.56      0.45      0.50        31\n",
      "              App Related       0.00      0.00      0.00        14\n",
      "             Cancellation       0.55      0.07      0.12       258\n",
      "           Career Related       0.00      0.00      0.00         7\n",
      "         Cash-Out Related       0.73      0.63      0.68       259\n",
      "                   Damage       0.07      0.05      0.06        19\n",
      "         Delivery Related       0.49      0.17      0.26       706\n",
      "Feedback/ Feature Request       0.29      0.19      0.23       176\n",
      "            General Query       0.37      0.18      0.24       551\n",
      "     Listing/Post Related       0.35      0.24      0.29        29\n",
      "         Marketing Emails       0.25      0.17      0.20         6\n",
      "            Offer Related       0.00      0.00      0.00         8\n",
      "            Ops Complaint       0.27      0.09      0.13        34\n",
      "                Ops Query       0.20      0.02      0.04        51\n",
      "            Order Related       0.54      0.86      0.67      4210\n",
      "                    Other       0.56      0.32      0.41       394\n",
      "         Packaging Issues       0.00      0.00      0.00         4\n",
      "          Payment Related       0.50      0.34      0.41       614\n",
      "           Pickup Related       0.62      0.44      0.51      1268\n",
      "       Post Sales queries       0.00      0.00      0.00        12\n",
      "        Pre-sales queries       0.00      0.00      0.00        41\n",
      "        Privacy Violation       0.00      0.00      0.00         3\n",
      "                 Referral       0.75      0.43      0.55        14\n",
      "             Refund Issue       0.00      0.00      0.00        26\n",
      "               Return/SBS       0.56      0.37      0.45       766\n",
      "              SBS Related       0.60      0.35      0.44       368\n",
      "     Serviceability Query       0.44      0.29      0.35        14\n",
      " Shipping Charges Queries       0.20      0.07      0.11        14\n",
      "           Shipping Label       0.46      0.12      0.19        51\n",
      "             Tech Related       0.30      0.21      0.24        87\n",
      "\n",
      "              avg / total       0.53      0.54      0.50     10035\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtoru\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "e.printConfusionMatrix(prediction, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some problems I found when I did classification was that many of the types are very similar to one another and are sometimes inconsistently used. For example, the tag \"Cancellation\" only started being used after 1/26. Before that, it used be in \"order related\". This might be a big reason for the low accuracy. Another might be because of the sheer amount of different types. When I reduce the number of types, the accuracy improves drastically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One library I found that could be used was TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Could use the following library for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "analysis = TextBlob('I like this. This is good!')\n",
    "analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I decided to build most of the sentiment analysis myself though in order to have more flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following methods were inspired by http://fjavieralba.com/basic-sentiment-analysis-with-python.html\n",
    "\n",
    "#splits the doc into a list of list of words taking out words with numbers in them\n",
    "#doc is a list of sentence strings\n",
    "    #[\"this is an example\", \"hi\"...]\n",
    "#returns a list of list of words\n",
    "    #[[\"this\", \"is\", \"an\", \"example\"], [\"hi\"] ...]\n",
    "def tokenize(doc):\n",
    "    tokenizedSentences = []\n",
    "    for sentence in doc:\n",
    "        originalTokens = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
    "        newTokens = []\n",
    "        for token in originalTokens:\n",
    "            if not any(char.isdigit() for char in token):\n",
    "                newTokens.append(token.lower())\n",
    "        tokenizedSentences.append(newTokens)\n",
    "    return tokenizedSentences\n",
    "    \n",
    "\n",
    "#sentences is a list of list of words\n",
    "    #[[\"this\", \"is\", \"an\", \"example\"], [\"hi\"] ...]\n",
    "#returns list of lists of tagged tokens [word, tag]\n",
    "    #[[['this',['DT']],['is',['VB']],['an', ['DT']],['example',['NN']]], [['hi', ['NN']]]]\n",
    "def tagPartOfSpeech(sentences):\n",
    "    progress = 0\n",
    "\n",
    "    taggedSentences=[]\n",
    "    for sentence in sentences:\n",
    "        taggedSentence = []\n",
    "        #print(sentence)\n",
    "        tags = nltk.pos_tag(sentence)\n",
    "        #print(tags)\n",
    "        for index in range(len(sentence)):\n",
    "            (word, tag) = tags[index]\n",
    "            #for tag in tags:\n",
    "            taggedSentence.append([word,[tag]])\n",
    "                    \n",
    "        taggedSentences.append(taggedSentence)\n",
    "        \n",
    "        progress+=1\n",
    "        if(progress%500==0):\n",
    "            print(progress)\n",
    "    \n",
    "    return taggedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numberToDo = 4000 #this variable specified how many descriptions I actually want to deal with\n",
    "    #dealing  with all takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizedSentences = tokenize(descriptions[:numberToDo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "taggedSentences = tagPartOfSpeech(tokenizedSentences[:numberToDo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionaries):\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for currDict in dictionaries:\n",
    "            for key in currDict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(currDict[key])\n",
    "                else:\n",
    "                    self.dictionary[key]=currDict[key]\n",
    "                    self.max_key_size = max (self.max_key_size, len(key))\n",
    "    \n",
    "    #tag all the sentences with pos, neg, incr, decr...\n",
    "    def tagSentences(self, sentences):\n",
    "        return [self.tagSentence(sentence) for sentence in sentences]\n",
    "\n",
    "    #tag 1 sentence\n",
    "    def tagSentence(self, sentence):\n",
    "        \n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][1]\n",
    "                        tagged_expression[1].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Positive words and negative words come from: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "\n",
    "p = ['positive']\n",
    "positiveWords = {'nice':p, 'awesome':p, 'cool':p, 'superb':p}\n",
    "posFile = open('positive-words.txt')\n",
    "for word in posFile:\n",
    "    positiveWords[word]=['positive']\n",
    "\n",
    "\n",
    "n=['negative']\n",
    "negativeWords = {'bad':n, 'uninspired':n, 'expensive':n, 'dissapointed':n, 'delay' : n}\n",
    "negFile = open('negative-words.txt')\n",
    "\n",
    "for word in negFile:\n",
    "    negativeWords[word]=['negative']\n",
    "\n",
    "i=['incr']\n",
    "incrWords = {'too':i, 'very':i, 'sorely':i, 'more':i}\n",
    "\n",
    "d=['decr']\n",
    "decrWords = {'barely':d, 'little':d, 'less':d}\n",
    "\n",
    "inverseWords = {'lack of':['inverse'],'not':['inverse']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Could just do it with positiveWords and negativeWords and not the other ones...\n",
    "\n",
    "#dicttagger = DictionaryTagger([positiveWords, negativeWords])\n",
    "#dicttagger = DictionaryTagger([positiveWords, negativeWords, incrWords, decrWords])\n",
    "dicttagger = DictionaryTagger([positiveWords, negativeWords, incrWords, decrWords, inverseWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sentences witht the added positive, negative... tags\n",
    "dict_tagged_sentences = dicttagger.tagSentences(taggedSentences[:numberToDo])\n",
    "#dict_tagged_sentences = dicttagger.tag([[['this',['DT']],['is',['VB']],['not', ['DT']],['nice',['NN']]], [['hi', ['NN']]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': \n",
    "        #print('p')\n",
    "        return 1\n",
    "    if sentiment == 'negative': \n",
    "        #print('n')\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):    \n",
    "    return sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_score(sentence_tokens, previous_token, acum_score):\n",
    "    if(len(sentence_tokens)>2000): #this is way too much already. shouldn't have any bigger. otherwise, the recursive stack limit will be reached\n",
    "        return 0\n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        #print(sentence_tokens)\n",
    "        current_token = sentence_tokens[0]\n",
    "        #print(current_token)\n",
    "        tags = current_token[1]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[1]\n",
    "            if 'incr' in previous_tags:\n",
    "                #print('incr')\n",
    "                token_score *= 2.0\n",
    "            elif 'decr' in previous_tags:\n",
    "                #print('decr')\n",
    "                token_score /= 2.0\n",
    "            elif 'inverse' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        #Sometimes hits recursion limit\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(review):\n",
    "    score = sum([sentence_score(sentence, None, 0.0) for sentence in review])\n",
    "    if(score>5):\n",
    "        score = 5\n",
    "    if(score<-5):\n",
    "        score = -5\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawSentiments = []\n",
    "for index in range(len(dict_tagged_sentences)):\n",
    "    rawSentiments.append(sentiment_score([dict_tagged_sentences[index]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of results\n",
    "Here, we look at the change in sentiment per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The dates are of form day/month/year time\n",
    "def getMonth (date):\n",
    "    first = date.find('/')\n",
    "    day = date[:first]\n",
    "    other = date[first+1:]\n",
    "    first = date.find('/')\n",
    "    return int(other[:first])\n",
    "\n",
    "#never used\n",
    "def getDayOfWeek (date):\n",
    "    first = date.find('/')\n",
    "    day = date[:first]\n",
    "    other = date[first+1:]\n",
    "    \n",
    "    first = date.find('/')\n",
    "    month = other[:first]\n",
    "    other = other[first+1:]\n",
    "    \n",
    "    year = other\n",
    "    \n",
    "    print(day)\n",
    "    print(month)\n",
    "    print(year)\n",
    "    theDate =datetime.datetime(year, month, day)\n",
    "    return theDate.weekday()\n",
    "\n",
    "months = []\n",
    "times = []\n",
    "for i in range(len(dict_tagged_sentences)):\n",
    "    #can change based on what stat\n",
    "    months.append(getMonth(createdTimes[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "results = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "times = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for index in range(len(months)):\n",
    "    #print(months[index])\n",
    "    #print(index)\n",
    "    results[months[index]-1] +=rawSentiments[index]\n",
    "    times[months[index]-1]+=1\n",
    "\n",
    "for i in range(len(m)):\n",
    "    if(results[i]!=0):\n",
    "        results[i] = results[i]/times[i]\n",
    "        \n",
    "resultingM=[0,1,2,3,4,5]#=[10,11,12,1,2]\n",
    "results = [results[8],results[9],results[10],results[11],results[0],results[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEKCAYAAAArYJMgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23e40011240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x23e401c70f0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.bar(resultingM, results, align = 'center', alpha = 0.5,tick_label =['September','October','November','December','January','February'])\n",
    "plt.show()\n",
    "plt.xlabel(\"months\")\n",
    "plt.ylabel(\"average sentence scores\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
